{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DFTStructureGenerator import DFThandle\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binol_3_3 = [1,2,3,13,18,19,20,21,23,24,25,26,28,29,30] + np.arange(122, 131).tolist()\n",
    "binol_4_4 = [62,63,66]\n",
    "binol_6_6 = [31,32,35,36,] + np.arange(44, 48).tolist()\n",
    "binol_7_7 = [51,52,56,57,60]\n",
    "binol_other = [0,85,91,121]\n",
    "all_binol_split = [binol_3_3, binol_4_4, binol_6_6, binol_7_7, binol_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_ligand = [1,3,4,5,6,9,10,11,12,14,15,16,17,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,44,45,46,48,49,50,51,52,54,56,57,58,59,61,62,63,64,65,66,67,68,69,70,\n",
    "71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,91,92,93,94,95,96,97,98,99,100,\n",
    "101,102,104,105,106,107,108,109,110,111,113,114,117,118,122,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,\n",
    "151,152,153,154,155,156,157,158,159,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205]\n",
    "banned_binol = [188, 195, 220, 676]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"Data\"\n",
    "csv_dir = os.path.join(data_dir, \"Iteration_2\", \"Result\")\n",
    "row_csv = \"Data_clear.csv\"\n",
    "target_csv = \"Data_clear_with_sites.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"Data/all_fp_map2.pkl\", 'rb')as f:\n",
    "    # rd_mf_map, rd_des_map, morgan_map, modred_map, acsf_3D_map, soap_3D_map, mbtr_3D_map, lmbtr_3D_map, qm_dict, area_dict = pickle.load(f)\n",
    "    qm_dict, area_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Full Reaction Space Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_csv = pd.read_csv(target_csv)\n",
    "binol_csv = smiles_csv.loc[smiles_csv['Type'] == 'Binol']\n",
    "ligand_csv = smiles_csv.loc[(smiles_csv['Type'] == 'Ligand_Box') | (smiles_csv['Type'] == 'Ligand_Other')]\n",
    "ligand_csv = ligand_csv.loc[np.isin(ligand_csv['Index'] - 1000, allowed_ligand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_csv = pd.read_csv(\"Data_clear_with_sites.csv\")\n",
    "binol_csv = smiles_csv.loc[smiles_csv['Type'] == 'Binol']\n",
    "ligand_csv = smiles_csv.loc[(smiles_csv['Type'] == 'Ligand_Box') | (smiles_csv['Type'] == 'Ligand_Other')]\n",
    "new_dict = {\"Binol\":{}, \"Ligand\":{}, \"Binol_smiles\":{},\"Ligand_smiles\":{}, \"Binol_conf_id\":{}, \"Ligand_conf_id\":{}, \"Binol_Sites\":{}, \"Ligand_Sites\":{}, \"Binol_G\":{}, \"Ligand_G\":{}}\n",
    "new_dict_id = 0\n",
    "for _, binol_row in binol_csv.iterrows():\n",
    "    for _, ligand_row in ligand_csv.iterrows():\n",
    "        if ligand_row['Index'] - 1000 not in allowed_ligand:\n",
    "            continue\n",
    "        new_dict[\"Binol\"][new_dict_id] = binol_row['Index']\n",
    "        new_dict[\"Ligand\"][new_dict_id] = ligand_row['Index']\n",
    "        new_dict[\"Binol_smiles\"][new_dict_id] = binol_row['Smiles']\n",
    "        new_dict[\"Ligand_smiles\"][new_dict_id] = ligand_row['Smiles']\n",
    "        new_dict[\"Binol_conf_id\"][new_dict_id] = binol_row['conf_id']\n",
    "        new_dict[\"Ligand_conf_id\"][new_dict_id] = ligand_row['conf_id']\n",
    "        new_dict[\"Binol_Sites\"][new_dict_id] = binol_row['Sites']\n",
    "        new_dict[\"Ligand_Sites\"][new_dict_id] = ligand_row['Sites']\n",
    "        new_dict[\"Binol_G\"][new_dict_id] = binol_row['G/Hatree']\n",
    "        new_dict[\"Ligand_G\"][new_dict_id] = ligand_row['G/Hatree']\n",
    "        new_dict_id += 1\n",
    "new_dict = pd.DataFrame(new_dict)\n",
    "new_dict.to_csv(\"Data/Iteration_2/Full_Space_20250925.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_csv = DFThandle.read_reaction_csv(\"Data/Full_Space_new.csv\")\n",
    "data_clear_csv = pd.read_csv(\"Data_clear_with_sites.csv\")\n",
    "fps = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(row['Smiles']), 2, nBits=2048) for row_id, row in data_clear_csv.iterrows()]\n",
    "mfps = np.array(fps) @ np.array(fps).T\n",
    "row_id_idx = {row[\"Index\"]: row_id for row_id, row in data_clear_csv.iterrows()}\n",
    "target_idx = [[row_id_idx[row[\"Binol\"]], row_id_idx[row[\"Ligand\"]]] for row_id, row in full_data_csv.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker\n",
    "from rdkit import DataStructs\n",
    "def distij(i,j):\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
    "    \n",
    "def distanceij(i,j):\n",
    "    B_i, L_i = target_idx[i]\n",
    "    B_j, L_j = target_idx[j]\n",
    "    return distij(B_i, B_j) + distij(L_i, L_j)#  * rate_a # + var_j * rate_b\n",
    "Times = 0      \n",
    "picker = MaxMinPicker()\n",
    "first_picker_idx = picker.LazyPick(distanceij, len(full_data_csv), 50, firstPicks=[])\n",
    "np.save(f'Data/Iteration_Data2/iter_{Times:05}.npy', first_picker_idx)\n",
    "full_data_csv.iloc[first_picker_idx].to_csv(f\"Data/Iteration_Data2/iter_{Times:05}_sameBinol.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Round(1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def distij(i,j):\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
    "\n",
    "def generate_similarity(B_i, L_i, B_j, L_j):\n",
    "    k = mfps[B_i, B_j] + mfps[L_i, L_j]\n",
    "    return k / (mfps[B_i, B_i] + mfps[L_i, L_i] + mfps[B_j, B_j] + mfps[L_j, L_j] - k)\n",
    "\n",
    "def distanceij(i,j):\n",
    "    B_i, L_i = target_idx[i]\n",
    "    B_j, L_j = target_idx[j]\n",
    "    var_i = this_uncertainty[i]\n",
    "    return distij(B_i, B_j) + distij(L_i, L_j) + var_i + avaliable(B_i, L_i) * 1000\n",
    "\n",
    "def get_uncertainty(first_data_csv):\n",
    "    train_X = DFThandle.descriptor_to_array(first_data_csv, None, [qm_dict, area_dict])\n",
    "    train_Y = first_data_csv['R'].to_numpy() - first_data_csv[\"S\"].to_numpy()\n",
    "    temp_train_X =  DFThandle.get_reverse_result(train_X)\n",
    "    temp_train_Y = train_Y * -1\n",
    "    train_X = np.concatenate([train_X, temp_train_X])\n",
    "    train_Y = np.append(train_Y, temp_train_Y)\n",
    "    \n",
    "    model = CatBoostRegressor(task_type=\"CPU\", iterations=10000, learning_rate=0.01, depth=6, verbose=0, random_state = 0, loss_function='RMSEWithUncertainty')\n",
    "    model.fit(train_X, train_Y)\n",
    "    new_desc_array = DFThandle.descriptor_to_array(full_data_csv, None, [qm_dict, area_dict])\n",
    "    new_result, new_uncertainty = model.predict(new_desc_array).T\n",
    "    return new_result, new_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = 5\n",
    "first_data_csv = DFThandle.read_reaction_csv(f\"Data/Iteration_2/Result/BINOL_result_sum_{Times-1:04}.csv\")\n",
    "first_idxs = np.load(f\"Data/Iteration_2/Iteration_Data/iter_{Times-1:05}.npy\")\n",
    "all_result, this_uncertainty = get_uncertainty(first_data_csv)\n",
    "full_data_csv['Pred_Value'] = all_result\n",
    "pre_select_id = first_idxs.tolist()\n",
    "picker = MaxMinPicker()\n",
    "picker_idx = picker.LazyPick(distanceij, len(full_data_csv), len(first_idxs) + 50, firstPicks=pre_select_id)\n",
    "np.save(f'Data/Iteration_2/Iteration_Data/iter_{Times:05}.npy', picker_idx)\n",
    "full_data_csv.iloc[picker_idx[-50:]].to_csv(f\"Data/Iteration_2/Iteration_Data/iter_{Times:05}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Round (6 - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = first_data_csv\n",
    "y = data_csv['R'].to_numpy() - data_csv[\"S\"].to_numpy()\n",
    "all_X = DFThandle.descriptor_to_array(data_csv, None, [qm_dict, area_dict])\n",
    "target = y\n",
    "all_pred = np.zeros(len(all_X))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "kf = list(kf.split(all_X))\n",
    "r2_split = []\n",
    "for train_ids, test_ids in kf:\n",
    "    model = CatBoostRegressor(iterations=10000, learning_rate=0.01, depth=6, verbose=0, random_state = 0)\n",
    "    train_X, train_Y = all_X[train_ids], target[train_ids]\n",
    "    temp_train_X =  DFThandle.get_reverse_result(train_X)\n",
    "    temp_train_Y = train_Y * -1\n",
    "    train_X_ = np.concatenate([train_X, temp_train_X])\n",
    "    train_Y_ = np.append(train_Y, temp_train_Y)\n",
    "    model.fit(train_X_, train_Y_)\n",
    "    y_pred = model.predict(all_X[test_ids])\n",
    "    all_pred[test_ids] = y_pred\n",
    "for binol_split_id, binol_split in enumerate(all_binol_split):\n",
    "    row_in_split = [id_ for id_, each in data_csv.iterrows() if each['Binol'] in binol_split]\n",
    "    r2_split.append(r2_score(target[row_in_split], all_pred[row_in_split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stratified_sample_allocation(n, weights):\n",
    "    weights = np.array(weights)\n",
    "    expected = n * weights\n",
    "    floors = np.floor(expected).astype(int)\n",
    "    fractions = expected - floors\n",
    "    remaining = n - np.sum(floors)\n",
    "\n",
    "    sorted_indices = np.argsort(-fractions)\n",
    "    \n",
    "    allocations = floors.copy()\n",
    "    allocations[sorted_indices[:remaining]] += 1\n",
    "    return allocations\n",
    "\n",
    "weight = (1 - np.array(r2_split)) * np.array([len(each) for each in all_binol_split])\n",
    "weight /= np.sum(weight)\n",
    "binol_sample_num = stratified_sample_allocation(50, weight)\n",
    "binol_sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliable(B_j, L_j):\n",
    "    if B_j in binol:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = 15\n",
    "first_idxs = np.load(f\"Data/Iteration_2/Iteration_Data/iter_{Times-1:05}.npy\")\n",
    "picker_idx = first_idxs.tolist()\n",
    "for binol, binol_sample in zip(all_binol_split, binol_sample_num.tolist()):\n",
    "    if binol_sample <= 0:\n",
    "        continue\n",
    "    picker = MaxMinPicker()\n",
    "    picker_idx = picker.LazyPick(distanceij, len(full_data_csv), len(picker_idx) + binol_sample, firstPicks=list(picker_idx))\n",
    "full_data_csv['Pred_Value'] = all_result\n",
    "np.save(f'Data/Iteration_2/Iteration_Data/iter_{Times:05}.npy', picker_idx)\n",
    "full_data_csv.iloc[list(picker_idx)[-np.sum(binol_sample_num):]].to_csv(f\"Data/Iteration_2/Iteration_Data/iter_{Times:05}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smiles_csv = pd.read_csv(\"Data_clear_with_sites.csv\")\n",
    "binol_csv = smiles_csv.loc[smiles_csv['Type'] == 'Binol']\n",
    "important_binol_id = np.arange(max(binol_csv['Index'].to_list())+1)\n",
    "# important_binol_id = [802,803,804]\n",
    "full_data_csv = pd.read_csv('Data/Iteration_2/Full_Space_20250925.csv')\n",
    "for id_ in range(14,15):\n",
    "    first_data_csv = DFThandle.read_reaction_csv(rf'Data\\Iteration_2\\Result\\BINOL_result_sum_{id_:04}.csv')\n",
    "    old_desc_array = DFThandle.descriptor_to_array(first_data_csv, None, [qm_dict, area_dict])\n",
    "    old_y = first_data_csv['R'].to_numpy() - first_data_csv[\"S\"].to_numpy()\n",
    "    model = CatBoostRegressor(task_type=\"CPU\", iterations=10000, learning_rate=0.01, depth=6, verbose=0, random_state = 0)\n",
    "    model.fit(old_desc_array, old_y)\n",
    "    new_desc_array = DFThandle.descriptor_to_array(full_data_csv, None, [qm_dict, area_dict])\n",
    "    new_y = model.predict(new_desc_array).T\n",
    "    full_data_csv['PredER'] = new_y\n",
    "    target_ids = []\n",
    "    for binol_id in important_binol_id:\n",
    "    # for binol_id in [24]:\n",
    "        temp_csv = full_data_csv.loc[full_data_csv['Binol'] == binol_id]\n",
    "        already_ligand = first_data_csv.loc[first_data_csv['Binol'] == binol_id]['Ligand'].to_list()\n",
    "        # temp_csv = full_data_csv.loc[full_data_csv['In Dataset'] == 0]\n",
    "        temp_csv['PredERAbs'] = np.abs(temp_csv['PredER'])\n",
    "        temp_csv = temp_csv.sort_values(by='PredERAbs', ascending=False)\n",
    "        temp_csv = temp_csv.loc[~np.isin(temp_csv[\"Ligand\"], already_ligand)]\n",
    "        target_ids +=[row_id for row_id, row in temp_csv.iterrows()][:10]\n",
    "        # temp_csv = temp_csv.sort_values(by='PredER', ascending=True)\n",
    "        # target_ids += [row_id for row_id, row in temp_csv.iterrows()][:3]\n",
    "    # target_ids = list(set(target_ids))\n",
    "    new_csv = full_data_csv.iloc[target_ids]\n",
    "    new_csv['PredE'] = (new_csv['PredER'] + 100) / 2\n",
    "    new_csv['PredR'] = (new_csv['PredER'] - 100) / 2\n",
    "    new_csv.to_csv(f\"Pred_Important_Binol_09_25_{id_:04}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_py3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
